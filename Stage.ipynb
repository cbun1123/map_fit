{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/cbun1123/map_fit/blob/main/Stage.ipynb",
      "authorship_tag": "ABX9TyNVwyGT3a5QTM1dzL2VQrtF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbun1123/map_fit/blob/main/Stage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -r /content/map_fit\n",
        "!git clone https://github.com/cbun1123/map_fit\n",
        "\n",
        "!unzip /content/map_fit/nrevol/X_train_1.zip -d /content/map_fit/X_train\n",
        "!unzip /content/map_fit/nrevol/X_train_2.zip -d /content/map_fit/X_train\n",
        "!unzip /content/map_fit/nrevol/X_train_3.zip -d /content/map_fit/X_train\n",
        "!unzip /content/map_fit/nrevol/X_train_4.zip -d /content/map_fit/X_train\n",
        "!unzip /content/map_fit/nrevol/X_train_5.zip -d /content/map_fit/X_train\n",
        "!unzip /content/map_fit/nrevol/X_train_6.zip -d /content/map_fit/X_train\n",
        "!unzip /content/map_fit/nrevol/X_train_7.zip -d /content/map_fit/X_train\n",
        "!unzip /content/map_fit/nrevol/X_train_8.zip -d /content/map_fit/X_train\n",
        "!unzip /content/map_fit/nrevol/X_train_9.zip -d /content/map_fit/X_train\n",
        "!unzip /content/map_fit/nrevol/X_train_10.zip -d /content/map_fit/X_train\n",
        "\n",
        "!unzip /content/map_fit/nrevol/X_test_1.zip -d /content/map_fit/X_test\n",
        "!unzip /content/map_fit/nrevol/X_test_2.zip -d /content/map_fit/X_test\n",
        "\n",
        "!rm /content/map_fit/*.zip"
      ],
      "metadata": {
        "id": "0mtWihytt32k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning\n",
        "!pip install tensorboardcolab\n",
        "!pip install torchmetrics\n",
        "%env PYTHONPATH=.:$PYTHONPATH"
      ],
      "metadata": {
        "id": "PyjF4ET6Ryta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow-HQUN3k9SA"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "n = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import gc\n",
        "import natsort\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.onnx\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import torchmetrics\n",
        "from torchmetrics.functional import *\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "Gj6-PAXEtQDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir='/content/lightning_logs'"
      ],
      "metadata": {
        "id": "863-WKKY3pU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training data\n",
        "\n",
        "filenames = glob.glob(\"/content/map_fit/X_train/*.tiff\")\n",
        "filenames = natsort.natsorted(filenames)\n",
        "X_train = [cv2.imread(img,-1) for img in filenames]\n",
        "\n",
        "y_train = np.load('/content/map_fit/nrevol/Y_train.npy')\n",
        "print(f'Training set : {np.shape(X_train)}')\n",
        "\n",
        "## Testing data\n",
        "\n",
        "filenames = glob.glob(\"/content/map_fit/X_test/*.tiff\")\n",
        "filenames = natsort.natsorted(filenames)\n",
        "X_test = [cv2.imread(img,-1) for img in filenames]\n",
        "\n",
        "y_test = np.load('/content/map_fit/nrevol/Y_test.npy')\n",
        "print(f'Testing set : {np.shape(X_test)}')"
      ],
      "metadata": {
        "id": "WHAHiJg7vmyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SIZE = 224\n",
        "\n",
        "# Define mask\n",
        "lin = np.linspace(-1,1,SIZE)\n",
        "[Xm,Ym] = np.meshgrid(lin,lin)\n",
        "idx = ((Xm**2+Ym**2)<1)\n",
        "idx = np.stack([idx,idx,idx], axis=0)\n",
        "idx_t = torch.from_numpy(idx)\n",
        "\n",
        "# Define datesets\n",
        "class imageDataset(Dataset):\n",
        "    def __init__(self,X,y):\n",
        "        'Initialization'\n",
        "        self.X = np.array(X).astype(np.float32) # /(2**16)\n",
        "        self.y = np.array(y).astype(np.float32) # /(2**16)\n",
        "        self.n_samples = self.X.shape[0]\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        X = self.X[index]\n",
        "        X = self.transform(X)\n",
        "        X = torch.mul(X,idx_t)\n",
        "\n",
        "        y = self.y[index]\n",
        "        y = torch.from_numpy(y)\n",
        "        sample = X,y\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "# Init datasets\n",
        "trainset = imageDataset(X_train,y_train)\n",
        "testset = imageDataset(X_test,y_test)\n",
        "\n",
        "# Garbage collection\n",
        "del X_train\n",
        "del y_train\n",
        "del X_test\n",
        "del y_test\n",
        "del filenames\n",
        "del Xm\n",
        "del Ym\n",
        "del lin\n",
        "del idx\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "YeaSHISWLPdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ae866d-f8e2-4133-b18d-ac5c4dd3a67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freezing weights:\n",
        "```\n",
        "for param in backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "self.net = backbone\n",
        "```\n",
        "\n",
        "Replacing last layer:\n",
        "```\n",
        "backbone.fc = nn.Linear(in_features=backbone.fc.in_features, out_features=2, bias=True)\n",
        "self.net = backbone\n",
        "```\n",
        "\n",
        "Adding new last layer:\n",
        "```\n",
        "added_layer = nn.Linear(in_features=backbone.fc.out_features, out_features=2, bias=True)\n",
        "self.net = nn.Sequential(backbone,nn.ReLU(inplace=True),added_layer)\n",
        "```"
      ],
      "metadata": {
        "id": "zasYanDzx5Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout testing\n",
        "~~~\n",
        "def append_dropout(model, rate):\n",
        "    for name, module in model.named_children():\n",
        "        if len(list(module.children())) > 0:\n",
        "            append_dropout(module,rate)\n",
        "        if isinstance(module, nn.ReLU):\n",
        "            module.register_forward_hook(lambda m, inp, out: F.dropout(out, p=rate, training=m.training))\n",
        "            setattr(model, name, new)\n",
        "~~~"
      ],
      "metadata": {
        "id": "xSF1B_WhGHHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model_name = \"shufflenet_v2_x1_0\" # shufflenet_v2_x1_0\n",
        "\n",
        "# hyperparameters\n",
        "BATCH_SIZE = 512\n",
        "num_epochs = 10\n",
        "val_check_interval = 0.5\n",
        "learning_rate = 0.001\n",
        "betas = [0.9,0.999] # Gradient decay factor, Squared\n",
        "eps = 1e-8\n",
        "weight_decay = 1e-3 # L2 Regularization\n",
        "step_size = 3 # Learn rate drop period\n",
        "gamma = 0.8 # Learn rate drop factor\n",
        "rate = 0.0 # Dropout rate, 0 for none\n",
        "\n",
        "# Defining model\n",
        "class LitModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # init a pretrained resnet\n",
        "        backbone = torch.hub.load('pytorch/vision:v0.10.0', model_name, pretrained=True)\n",
        "        backbone.fc = nn.Linear(in_features=backbone.fc.in_features, out_features=2, bias=True)\n",
        "        self.net = backbone\n",
        "\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(dataset=trainset, batch_size=self.batch_size)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(dataset=testset, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(dataset=testset, batch_size=self.batch_size)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input, labels = batch\n",
        "        output = self.net(input)\n",
        "        loss = mean_squared_log_error(output,labels) # mean_squared_log_error(output,labels) # F.mse_loss(output,labels)\n",
        "\n",
        "        # Logging to TensorBoard by default\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input, labels = batch\n",
        "        output = self.net(input)\n",
        "        loss = mean_squared_log_error(output,labels)\n",
        "\n",
        "        # Logging to TensorBoard by default\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input, labels = batch\n",
        "        output = self.net(input)\n",
        "        loss = torch.abs(torch.div(output-labels,output)).mean()\n",
        "        self.log(\"test_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr, alpha=0.86, eps=eps, weight_decay=weight_decay, momentum=0.9)\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=step_size,gamma=gamma)\n",
        "        return [optimizer], [lr_scheduler]"
      ],
      "metadata": {
        "id": "Vl6yhwY9coxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# model init\n",
        "model = LitModel() #.load_from_checkpoint(\"/content/checkpoints/last.ckpt\")\n",
        "\n",
        "# logger\n",
        "logger = pl.loggers.TensorBoardLogger(save_dir=os.getcwd(), version=1, name=\"lightning_logs\")\n",
        "\n",
        "# callbacks\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=\"/content/checkpoints\",\n",
        "                                      save_last=True,\n",
        "                                      save_top_k=1,\n",
        "                                      monitor=\"val_loss\",\n",
        "                                      mode=\"min\",\n",
        "                                      filename=model_name+\"-{epoch:02d}-{val_loss:.5f}\"\n",
        "                                      )\n",
        "\n",
        "stagnate_callback = pl.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                               mode=\"min\",\n",
        "                                               check_finite=True,\n",
        "                                               patience=np.ceil(num_epochs/val_check_interval)\n",
        "                                               )\n",
        "swa_callback = pl.callbacks.StochasticWeightAveraging(swa_lrs=1e-4)\n",
        "\n",
        "# Defining trainer\n",
        "trainer = pl.Trainer(accelerator=\"gpu\",             # CPU, GPU or TPU\n",
        "                     val_check_interval=val_check_interval,\n",
        "                     auto_lr_find=False,            # True or False   \n",
        "                     auto_scale_batch_size=None,    # None or \"binsearch\"\n",
        "                     deterministic=False,           # True or False\n",
        "                     fast_dev_run=False,            # True or False or Epoch count\n",
        "                     logger=logger,                 # logger or False\n",
        "                     max_epochs=1000,               # -1 for Infinite\n",
        "                     precision=32,                  # Default 32\n",
        "                     profiler=None,                 # None, \"simple\" or \"advanced\"\n",
        "                     enable_checkpointing=True,     # True or False\n",
        "                     callbacks=[checkpoint_callback, stagnate_callback, swa_callback],\n",
        "                     gradient_clip_val=0.25,\n",
        "                     log_every_n_steps=1\n",
        "                     #, overfit_batches=1, detect_anomaly=True\n",
        "                     )                  \n",
        "\n",
        "# Autotune hyperparameters\n",
        "trainer.tune(model=model)\n",
        "\n",
        "# Training\n",
        "trainer.fit(model=model)"
      ],
      "metadata": {
        "id": "a2eqAEcNKafw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading best checkpoint\n",
        "t_loss = trainer.test()[0].get('test_loss')\n",
        "\n",
        "# save for inference\n",
        "filename = f'/content/{model_name}_{t_loss:.3f}_inf.onnx'\n",
        "\n",
        "# Input to the model\n",
        "x = torch.randn(1, 3, SIZE, SIZE, requires_grad=True)\n",
        "\n",
        "# Export the model\n",
        "model.to_onnx(filename, x, export_params=True)"
      ],
      "metadata": {
        "id": "v45W0Eh1nvRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}