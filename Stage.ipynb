{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/cbun1123/map_fit/blob/main/Stage.ipynb",
      "authorship_tag": "ABX9TyPeoVU0Hh0uTbvcOlrNiJ7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbun1123/map_fit/blob/main/Stage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -r /content/map_fit\n",
        "!git clone https://github.com/cbun1123/map_fit\n",
        "\n",
        "!unzip /content/map_fit/8bit/X_train_1.zip -d /content/map_fit/8bit/X_train\n",
        "!unzip /content/map_fit/8bit/X_train_2.zip -d /content/map_fit/8bit/X_train\n",
        "!unzip /content/map_fit/8bit/X_train_3.zip -d /content/map_fit/8bit/X_train\n",
        "\n",
        "!unzip /content/map_fit/8bit/X_test_1.zip -d /content/map_fit/8bit/X_test\n",
        "\n",
        "!rm /content/map_fit/*.zip"
      ],
      "metadata": {
        "id": "0mtWihytt32k"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!pip install cloud-tpu-client==0.10 torch==1.12.0 https://storage.googleapis.com/tpu-pytorch/wheels/cuda/112/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl --force-reinstall\n",
        "#!pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@master --upgrade\n",
        "!pip install pytorch-lightning #==1.5.10\n",
        "!pip install tensorboardcolab\n",
        "!pip install torchmetrics\n",
        "!pip install \"ray[tune]\"\n",
        "!pip install GPUtil\n",
        "%env PYTHONPATH=.:$PYTHONPATH"
      ],
      "metadata": {
        "id": "PyjF4ET6Ryta"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ow-HQUN3k9SA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "571ea275-08fe-4e48-ca7e-31046cd88d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep  2 11:43:53 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "n = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import gc\n",
        "import natsort\n",
        "import logging\n",
        "import copy\n",
        "\n",
        "from IPython.utils import io\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.onnx\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import ray\n",
        "from ray import air,tune\n",
        "from ray.tune.integration.pytorch_lightning import TuneReportCallback,TuneReportCheckpointCallback\n",
        "\n",
        "import GPUtil\n",
        "\n",
        "import torchmetrics\n",
        "from torchmetrics.functional import *\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "#import torch_xla\n",
        "#import torch_xla.core.xla_model as xm"
      ],
      "metadata": {
        "id": "Gj6-PAXEtQDT",
        "cellView": "form"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir='/content/lightning_logs'"
      ],
      "metadata": {
        "id": "863-WKKY3pU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load dataset\n",
        "#@markdown Loads datasets and creates training and testing dataloaders\n",
        "SIZE = 224\n",
        "\n",
        "# Define circular mask for RGB tensor image\n",
        "lin = np.linspace(-1,1,SIZE)\n",
        "[Xm,Ym] = np.meshgrid(lin,lin)\n",
        "idx = ((Xm**2+Ym**2)<1)\n",
        "idx = np.stack([idx,idx,idx], axis=0)\n",
        "idx_t = torch.from_numpy(idx)\n",
        "\n",
        "# Define custom dataset class inheriting from torch.Dataset\n",
        "class imageDataset(Dataset):\n",
        "    def __init__(self,X,y,Z):\n",
        "        'Initialization'\n",
        "        # Load data\n",
        "        self.X = np.array(X).astype(np.float32)\n",
        "        self.y = np.array(y).astype(np.float32)\n",
        "        self.Z = np.array(Z).astype(np.float32)\n",
        "\n",
        "        # Define dataset size\n",
        "        self.n_samples = self.X.shape[0]\n",
        "\n",
        "        # Define target distribution\n",
        "        mean = torch.tensor([0.5, 0.5, 0.5]).view(3,1,1)\n",
        "        std = torch.tensor([0.5, 0.5, 0.5]).view(3,1,1)\n",
        "\n",
        "        # Define corrected distribution for masked image\n",
        "        fact = torch.div(SIZE**2,idx_t.sum(dim=[1,2])).view(3,1,1)\n",
        "        corr_mean = torch.mul(fact,mean)\n",
        "        corr_std = torch.sqrt(torch.mul(fact,torch.pow(std,2)) - torch.mul(torch.mul(fact,fact-1),torch.pow(mean,2)))\n",
        "\n",
        "        # Define transformations\n",
        "        lambdaMask = lambda T: torch.mul(T,idx_t)\n",
        "        lambdaNorm = lambda T: torch.div(T-T.mean(dim=[1,2]).view(3,1,1),T.std(dim=[1,2]).view(3,1,1))\n",
        "        lambdaScale = lambda T: torch.mul(T,corr_std) + corr_mean\n",
        "\n",
        "        # Define global transformation\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            #transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambdaMask),\n",
        "            transforms.Lambda(lambdaNorm),\n",
        "            transforms.Lambda(lambdaScale),\n",
        "            transforms.Lambda(lambdaMask),\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        X = self.X[index]\n",
        "\n",
        "        # Rescale sample to original values\n",
        "        #X = (X/255)*(self.Z[index,1]-self.Z[index,0]) + self.Z[index,0]\n",
        "\n",
        "        # Apply transforms\n",
        "        X = self.transform(X)\n",
        "\n",
        "        # Load and format labels\n",
        "        y = self.y[index]\n",
        "        y = torch.from_numpy(y)\n",
        "\n",
        "        # Package sample\n",
        "        sample = X,y\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        'Dataset length'\n",
        "        return self.n_samples\n",
        "\n",
        "    def getData(self):\n",
        "        'Returns all data labels'\n",
        "\n",
        "        # Load all images\n",
        "        #X = torch.stack([torch.mul(self.transform(self.X[i]),idx_t) for i in range(len(self.y))])\n",
        "\n",
        "        # Load all labels\n",
        "        y = torch.from_numpy(self.y)\n",
        "        return y\n",
        "\n",
        "## Training data\n",
        "train_num = 3000 #@param {type:\"slider\", min:0, max:10000, step:1}\n",
        "# Load training images\n",
        "filenames = glob.glob(\"/content/map_fit/8bit/X_train/*.jpg\")\n",
        "filenames = natsort.natsorted(filenames)\n",
        "X_train = [cv2.imread(img,-1) for img in filenames[0:train_num]] # cv2.imread(img)\n",
        "\n",
        "# Load training labels\n",
        "y_train = np.load('/content/map_fit/8bit/Y_train.npy')[0:train_num]\n",
        "Z_train = np.load('/content/map_fit/8bit/Z_train.npy')[0:train_num]\n",
        "\n",
        "## Testing data\n",
        "test_num = 300 #@param {type:\"slider\", min:0, max:2000, step:1}\n",
        "# Load testing images\n",
        "filenames = glob.glob(\"/content/map_fit/8bit/X_test/*.jpg\")\n",
        "filenames = natsort.natsorted(filenames)\n",
        "X_test = [cv2.imread(img,-1) for img in filenames[0:test_num]]\n",
        "\n",
        "# Load training labels\n",
        "y_test = np.load('/content/map_fit/8bit/Y_test.npy')[0:test_num]\n",
        "Z_test = np.load('/content/map_fit/8bit/Z_test.npy')[0:test_num]\n",
        "\n",
        "# Initialize datasets\n",
        "trainset = imageDataset(X_train,y_train,Z_train)\n",
        "testset = imageDataset(X_test,y_test,Z_test)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f'Training set : {np.shape(X_train)}')\n",
        "print(f'Testing set : {np.shape(X_test)}')\n",
        "\n",
        "\n",
        "# Garbage collection\n",
        "del X_train\n",
        "del y_train\n",
        "del X_test\n",
        "del y_test\n",
        "del filenames\n",
        "del Xm\n",
        "del Ym\n",
        "del lin\n",
        "del idx\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "YeaSHISWLPdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c9f11c8-0ba7-48c0-b5d2-af6072183d21",
        "cellView": "form"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set : (3000, 224, 224)\n",
            "Testing set : (300, 224, 224)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freezing weights:\n",
        "```\n",
        "for param in backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "self.net = backbone\n",
        "```\n",
        "\n",
        "Replacing last layer:\n",
        "```\n",
        "backbone.fc = nn.Linear(in_features=backbone.fc.in_features, out_features=3, bias=True)\n",
        "self.net = backbone\n",
        "```\n",
        "\n",
        "Adding new last layer:\n",
        "```\n",
        "added_layer = nn.Linear(in_features=backbone.fc.out_features, out_features=3, bias=True)\n",
        "self.net = nn.Sequential(backbone,nn.ReLU(inplace=True),added_layer)\n",
        "```"
      ],
      "metadata": {
        "id": "zasYanDzx5Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Defining model and non-tuned hyperparameters\n",
        "# Define model\n",
        "model_name = \"squeezenet1_1\" #@param {type:\"string\"}\n",
        "\n",
        "# Define hyperparameters\n",
        "BATCH_SIZE = 256 #@param {type:\"integer\"} # TPU has very little memory, 64 max\n",
        "\n",
        "#learning_rate = 0.0015\n",
        "#weight_decay = 1e-6\n",
        "#dropout_rate = 0.5\n",
        "#swa_lrs = 1e-6\n",
        "\n",
        "betas = [0.9,0.999]\n",
        "eps = 1e-8\n",
        "\n",
        "#@markdown Learning rate scheduler period\n",
        "step_size = 5 #@param {type:\"integer\"}\n",
        "#@markdown Learning rate scheduler factor\n",
        "gamma = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "# Define pyTorch-Lightning model class inheriting from LightningModule class\n",
        "class LitModel(pl.LightningModule):\n",
        "    def __init__(self,config,TrainLoader,TestLoader):\n",
        "        'Initialize module'\n",
        "        # Inherit initialization from LightningModule class\n",
        "        super().__init__()\n",
        "        # Define internal hyperparameters for tuning\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.lr = config[\"learning_rate\"]\n",
        "        self.wd = config[\"weight_decay\"]\n",
        "        self.p = config[\"dropout_rate\"]\n",
        "\n",
        "        self.TrainLoader = copy.deepcopy(TrainLoader)\n",
        "        self.TestLoader = copy.deepcopy(TestLoader)\n",
        "\n",
        "        # Initialize a pretrained network\n",
        "        backbone = torch.hub.load('pytorch/vision:v0.10.0', model_name, pretrained=True)\n",
        "        #backbone = torchvision.models.shufflenet_v2_x1_0(pretrained=True)\n",
        "\n",
        "        # Custom dropout rate (default 0.5)\n",
        "        backbone.classifier[0] = nn.Dropout(p=self.p, inplace=False)\n",
        "\n",
        "        # Add fully-connected last layer for 3 parameter regression\n",
        "        added_layer = nn.Linear(in_features=1000, out_features=3, bias=True)\n",
        "        self.net = nn.Sequential(backbone,added_layer)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        'Training optimizers definition'\n",
        "        # Backpropagation optimize\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, betas=betas, eps=eps, weight_decay=self.wd)\n",
        "        #optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr, alpha=0.86, eps=eps, weight_decay=self.wd, momentum=0.9)\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=step_size,gamma=gamma)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def forward(self,x):\n",
        "        'Forward function'\n",
        "        return self.net(x)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        'Internal training dataloader'\n",
        "        return self.TrainLoader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        'Internal validation dataloader'\n",
        "        return self.TestLoader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        'Internal testing dataloader'\n",
        "        return self.TestLoader\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        'Internal prediction dataloader'\n",
        "        return self.TestLoader\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        'Training loop step'\n",
        "        # Get data from batch\n",
        "        input, labels = batch\n",
        "        # Compute predictions\n",
        "        output = self.net(input)\n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(output,labels) # mean_squared_log_error(output,labels) # F.mse_loss(output,labels)\n",
        "        # Logging to TensorBoard\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        'Validation loop step'\n",
        "        # Get data from batch\n",
        "        input, labels = batch\n",
        "        # Compute predictions\n",
        "        output = self.net(input)\n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(output,labels)\n",
        "        # Logging to TensorBoard\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack(outputs).mean().item()\n",
        "        self.log(\"ptl/val_loss\", avg_loss)\n",
        "        self.log(\"epoch\", np.float32(self.current_epoch))\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        'Testing loop step'\n",
        "        # Get data from batch\n",
        "        input, labels = batch\n",
        "        # Compute predictions\n",
        "        output = self.net(input)\n",
        "        # Compute loss\n",
        "        loss = torch.std(labels-output, dim=0).mean()\n",
        "        # Logging to TensorBoard\n",
        "        self.log(\"test_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        'Prediction loop step'\n",
        "        # Get data from batch\n",
        "        input, labels = batch\n",
        "        # Compute predictions\n",
        "        return self.net(input)"
      ],
      "metadata": {
        "id": "Vl6yhwY9coxM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Raytune hyperparameter tuning\n",
        "\n",
        "# Garbage collection\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "n_trials = 10 #@param {type:\"integer\"}\n",
        "max_epochs = 1 #@param {type:\"integer\"}\n",
        "#@markdown Patience\n",
        "grace_period = 1 #@param {type:\"integer\"}\n",
        "num_epochs = 1 #@param {type:\"integer\"}\n",
        "val_check_interval = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "TrainLoader = DataLoader(dataset=trainset, batch_size=BATCH_SIZE)\n",
        "TestLoader = DataLoader(dataset=testset, batch_size=BATCH_SIZE)\n",
        "\n",
        "def train_network(config,TrainLoader,TestLoader):\n",
        "    # Garbage collection\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    logger = pl.loggers.TensorBoardLogger(save_dir=os.getcwd(), name=\"\", version=\".\"),\n",
        "\n",
        "\n",
        "    metrics = {\"val_loss\": \"ptl/val_loss\", \"epoch\": \"epoch\"}\n",
        "    tuning_callback = TuneReportCheckpointCallback(\n",
        "                metrics=metrics,\n",
        "                filename=\"checkpoint\",\n",
        "                on=\"validation_end\")\n",
        "\n",
        "    stagnate_callback = pl.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        mode=\"min\",\n",
        "        check_finite=True,\n",
        "        patience=np.ceil(num_epochs/val_check_interval)\n",
        "        )\n",
        "    \n",
        "    swa_callback = pl.callbacks.StochasticWeightAveraging(config[\"swa_lrs\"])\n",
        "    \n",
        "    \n",
        "    trainer = pl.Trainer(\n",
        "        accelerator=\"auto\",             # CPU, GPU or TPU\n",
        "        logger=logger,\n",
        "        val_check_interval=val_check_interval,\n",
        "        max_epochs=max_epochs,         # -1 for Infinite\n",
        "        enable_checkpointing=True,     # True or False\n",
        "        callbacks=[tuning_callback, swa_callback],\n",
        "        gradient_clip_val=0.25,\n",
        "        log_every_n_steps=1,\n",
        "        enable_progress_bar=True,\n",
        "        enable_model_summary=False,\n",
        "        detect_anomaly=True\n",
        "        )  \n",
        "\n",
        "    model = LitModel(config,TrainLoader,TestLoader)\n",
        "    #tune.utils.wait_for_gpu()\n",
        "    trainer.fit(model=model)\n",
        "\n",
        "    # Garbage collection\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "config = {\n",
        " \"learning_rate\": tune.loguniform(1e-6, 1e-2),\n",
        " \"weight_decay\": tune.loguniform(1e-8, 1e-2),\n",
        " \"dropout_rate\": tune.uniform(0.1, 0.6),\n",
        " \"swa_lrs\": tune.loguniform(1e-7,1e-2)\n",
        "}\n",
        "\n",
        "scheduler = tune.schedulers.AsyncHyperBandScheduler(\n",
        "    max_t=np.ceil(max_epochs/val_check_interval),\n",
        "    grace_period=np.ceil(grace_period/val_check_interval),\n",
        "    reduction_factor=4)\n",
        "\n",
        "trainable = tune.with_parameters(\n",
        "    train_network,\n",
        "    TrainLoader=TrainLoader,\n",
        "    TestLoader=TestLoader) #, args\n",
        "\n",
        "reporter = tune.CLIReporter(\n",
        "        parameter_columns=[\"learning_rate\", \"weight_decay\", \"dropout_rate\", \"swa_lrs\"],\n",
        "        metric_columns=[\"val_loss\",\"epoch\"])\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    tune.with_resources(\n",
        "        trainable,\n",
        "        resources={\"cpu\": 2, \"gpu\": 1},\n",
        "    ),\n",
        "    tune_config=tune.TuneConfig(\n",
        "        metric=\"val_loss\",\n",
        "        mode=\"min\",\n",
        "        scheduler=scheduler,\n",
        "        num_samples=n_trials,\n",
        "    ),\n",
        "    run_config=air.RunConfig(\n",
        "        name=\"train_network\",\n",
        "        progress_reporter=reporter,\n",
        "    ),\n",
        "    param_space=config,\n",
        ")\n",
        "\n",
        "results = tuner.fit()\n",
        "\n",
        "print(\"Best hyperparameters found were: \", results.get_best_result().config)"
      ],
      "metadata": {
        "id": "Dye8BmrJ0s-b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06cf2fd7-9d72-4625-9952-7cd87e8d6ac3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-09-02 12:29:38 (running for 00:00:00.31)\n",
            "Memory usage on this node: 8.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00000 | RUNNING  | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "| train_network_e9376_00001 | PENDING  |                 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m /usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m   f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m /usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m   warnings.warn(msg)\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:248: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m   \"The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\"\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:251: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:277: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m   f\"The `Callback.{hook}` hook was deprecated in v1.6 and\"\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:277: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m   f\"The `Callback.{hook}` hook was deprecated in v1.6 and\"\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:286: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m   f\"The `Callback.{hook}` hook was deprecated in v1.6 and\"\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:286: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m   f\"The `Callback.{hook}` hook was deprecated in v1.6 and\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-09-02 12:29:47 (running for 00:00:08.72)\n",
            "Memory usage on this node: 10.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00000 | RUNNING  | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "| train_network_e9376_00001 | PENDING  |                 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "\n",
            "\n",
            "Sanity Checking: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m \rSanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\rSanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:29:52 (running for 00:00:13.74)\n",
            "Memory usage on this node: 10.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00000 | RUNNING  | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "| train_network_e9376_00001 | PENDING  |                 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:29:57 (running for 00:00:18.74)\n",
            "Memory usage on this node: 11.9/12.7 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00000 | RUNNING  | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "| train_network_e9376_00001 | PENDING  |                 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "\n",
            "\n",
            "Training: 0it [00:00, ?it/s]\n",
            "Epoch 0:   0%|          | 0/16 [00:00<?, ?it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_network pid=3987)\u001b[0m Swapping scheduler `StepLR` for `SWALR`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:02 (running for 00:00:23.77)\n",
            "Memory usage on this node: 12.0/12.7 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00000 | RUNNING  | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "| train_network_e9376_00001 | PENDING  |                 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:07 (running for 00:00:28.81)\n",
            "Memory usage on this node: 12.1/12.7 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00000 | RUNNING  | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "| train_network_e9376_00001 | PENDING  |                 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:12 (running for 00:00:33.91)\n",
            "Memory usage on this node: 12.3/12.7 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00000 | RUNNING  | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "| train_network_e9376_00001 | PENDING  |                 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:17 (running for 00:00:39.41)\n",
            "Memory usage on this node: 12.3/12.7 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00000 | RUNNING  | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "| train_network_e9376_00001 | PENDING  |                 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:22 (running for 00:00:44.48)\n",
            "Memory usage on this node: 12.3/12.7 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00000 | RUNNING  | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "| train_network_e9376_00001 | PENDING  |                 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-09-02 12:30:26,300\tWARNING worker.py:1814 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe93b4465ed9bd13e9c009c2101000000 Worker ID: 63ecf80d2c9ebaa26b75bf580759e7c2fce617b6c92be6920b302816 Node ID: 7a0b66bb18f27ffdb00c4f85618285d97296f3798fc4bead73b01be9 Worker IP address: 172.28.0.2 Worker port: 44435 Worker PID: 3987 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "2022-09-02 12:30:26,321\tERROR trial_runner.py:980 -- Trial train_network_e9376_00000: Error processing event.\n",
            "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/execution/ray_trial_executor.py\", line 989, in get_next_executor_event\n",
            "    future_result = ray.get(ready_future)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/worker.py\", line 2277, in get\n",
            "    raise value\n",
            "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ImplicitFunc\n",
            "\tactor_id: e93b4465ed9bd13e9c009c2101000000\n",
            "\tpid: 3987\n",
            "\tnamespace: 3f7261fb-48a8-4f0f-b33f-c8664a642c4e\n",
            "\tip: 172.28.0.2\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_network_e9376_00000:\n",
            "  date: 2022-09-02_12-29-42\n",
            "  experiment_id: 5e630ce1660a4d6295d7dd1f42b9cbc3\n",
            "  hostname: 040cda79e45a\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3987\n",
            "  timestamp: 1662121782\n",
            "  trial_id: e9376_00000\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:32 (running for 00:00:53.69)\n",
            "Memory usage on this node: 8.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (1 ERROR, 8 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00001 | RUNNING  | 172.28.0.2:4088 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "| train_network_e9376_00000 | ERROR    | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "Number of errored trials: 1\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                |   # failures | error file                                                                                                                                                            |\n",
            "|---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_network_e9376_00000 |            1 | /root/ray_results/train_network/train_network_e9376_00000_0_dropout_rate=0.3022,learning_rate=0.0000,swa_lrs=0.0001,weight_decay=0.0006_2022-09-02_12-29-38/error.txt |\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:40 (running for 00:01:02.30)\n",
            "Memory usage on this node: 8.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (1 ERROR, 8 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00001 | RUNNING  | 172.28.0.2:4088 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "| train_network_e9376_00000 | ERROR    | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "Number of errored trials: 1\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                |   # failures | error file                                                                                                                                                            |\n",
            "|---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_network_e9376_00000 |            1 | /root/ray_results/train_network/train_network_e9376_00000_0_dropout_rate=0.3022,learning_rate=0.0000,swa_lrs=0.0001,weight_decay=0.0006_2022-09-02_12-29-38/error.txt |\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:45 (running for 00:01:07.30)\n",
            "Memory usage on this node: 8.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (1 ERROR, 8 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00001 | RUNNING  | 172.28.0.2:4088 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "| train_network_e9376_00000 | ERROR    | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "Number of errored trials: 1\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                |   # failures | error file                                                                                                                                                            |\n",
            "|---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_network_e9376_00000 |            1 | /root/ray_results/train_network/train_network_e9376_00000_0_dropout_rate=0.3022,learning_rate=0.0000,swa_lrs=0.0001,weight_decay=0.0006_2022-09-02_12-29-38/error.txt |\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:50 (running for 00:01:12.31)\n",
            "Memory usage on this node: 8.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (1 ERROR, 8 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00001 | RUNNING  | 172.28.0.2:4088 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "| train_network_e9376_00000 | ERROR    | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "Number of errored trials: 1\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                |   # failures | error file                                                                                                                                                            |\n",
            "|---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_network_e9376_00000 |            1 | /root/ray_results/train_network/train_network_e9376_00000_0_dropout_rate=0.3022,learning_rate=0.0000,swa_lrs=0.0001,weight_decay=0.0006_2022-09-02_12-29-38/error.txt |\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-09-02 12:30:54,653\tWARNING tune.py:687 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:55 (running for 00:01:17.31)\n",
            "Memory usage on this node: 8.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (1 ERROR, 8 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00001 | RUNNING  | 172.28.0.2:4088 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "| train_network_e9376_00000 | ERROR    | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "Number of errored trials: 1\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                |   # failures | error file                                                                                                                                                            |\n",
            "|---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_network_e9376_00000 |            1 | /root/ray_results/train_network/train_network_e9376_00000_0_dropout_rate=0.3022,learning_rate=0.0000,swa_lrs=0.0001,weight_decay=0.0006_2022-09-02_12-29-38/error.txt |\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-09-02 12:30:55 (running for 00:01:17.32)\n",
            "Memory usage on this node: 8.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.3 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_network\n",
            "Number of trials: 10/10 (1 ERROR, 8 PENDING, 1 RUNNING)\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "| Trial name                | status   | loc             |   learning_rate |   weight_decay |   dropout_rate |     swa_lrs |\n",
            "|---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------|\n",
            "| train_network_e9376_00001 | RUNNING  | 172.28.0.2:4088 |     0.00518233  |    1.21905e-08 |       0.163853 | 2.30214e-06 |\n",
            "| train_network_e9376_00002 | PENDING  |                 |     0.00014441  |    0.00430729  |       0.138704 | 5.65978e-06 |\n",
            "| train_network_e9376_00003 | PENDING  |                 |     1.05249e-06 |    2.06775e-06 |       0.167275 | 3.80054e-07 |\n",
            "| train_network_e9376_00004 | PENDING  |                 |     0.00235649  |    0.00185674  |       0.235766 | 0.000149204 |\n",
            "| train_network_e9376_00005 | PENDING  |                 |     4.81543e-06 |    1.84778e-08 |       0.550502 | 0.000208297 |\n",
            "| train_network_e9376_00006 | PENDING  |                 |     0.000792598 |    1.07147e-06 |       0.520705 | 0.00609172  |\n",
            "| train_network_e9376_00007 | PENDING  |                 |     0.00447972  |    2.99949e-05 |       0.495096 | 0.000242205 |\n",
            "| train_network_e9376_00008 | PENDING  |                 |     1.4198e-06  |    0.00191691  |       0.504762 | 3.99385e-06 |\n",
            "| train_network_e9376_00009 | PENDING  |                 |     3.39967e-05 |    0.000764871 |       0.289883 | 0.000925447 |\n",
            "| train_network_e9376_00000 | ERROR    | 172.28.0.2:3987 |     8.78689e-06 |    0.000609752 |       0.30224  | 0.000106867 |\n",
            "+---------------------------+----------+-----------------+-----------------+----------------+----------------+-------------+\n",
            "Number of errored trials: 1\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                |   # failures | error file                                                                                                                                                            |\n",
            "|---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_network_e9376_00000 |            1 | /root/ray_results/train_network/train_network_e9376_00000_0_dropout_rate=0.3022,learning_rate=0.0000,swa_lrs=0.0001,weight_decay=0.0006_2022-09-02_12-29-38/error.txt |\n",
            "+---------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-872d6583eb71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best hyperparameters found were: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/tuner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_ray_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 raise TuneError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/impl/tuner_internal.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_restored\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mparam_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/impl/tuner_internal.py\u001b[0m in \u001b[0;36m_fit_internal\u001b[0;34m(self, trainable, param_space)\u001b[0m\n\u001b[1;32m    379\u001b[0m         }\n\u001b[1;32m    380\u001b[0m         analysis = run(\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         )\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote)\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0mincomplete_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/execution/trial_runner.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0;34m\"\"\"Cleanup trials and callbacks.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_experiment_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/execution/trial_runner.py\u001b[0m in \u001b[0;36mcleanup_trials\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/execution/ray_trial_executor.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self, trials)\u001b[0m\n\u001b[1;32m    834\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_force_trial_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m             \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_futures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2466\u001b[0;31m             \u001b[0mfetch_local\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2467\u001b[0m         )\n\u001b[1;32m   2468\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial = results.best_trial  # Get best trial\n",
        "best_config = results.best_config  # Get best trial's hyperparameters\n",
        "best_checkpoint = results.best_checkpoint  # Get best trial's best checkpoint\n",
        "best_result = results.best_result  # Get best trial's last results"
      ],
      "metadata": {
        "id": "9xCh-4FhMoZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Training\n",
        "# Garbage collection\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Model initialization, possibly by checkpoint\n",
        "#@markdown Leave empty if no checkpoint\n",
        "ckpt_name = \"\" #@param {type:\"string\"}\n",
        "if ckpt_name == \"\":\n",
        "    model = LitModel()\n",
        "else:\n",
        "    model = LitModel().load_from_checkpoint(\"/content/checkpoints/\" + ckpt_name + \".ckpt\")\n",
        "\n",
        "\n",
        "# Define TensorBoard logger\n",
        "logger = pl.loggers.TensorBoardLogger(save_dir=os.getcwd(), version=n, name=\"lightning_logs\")\n",
        "n += 1\n",
        "\n",
        "# Define training callbacks\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=\"/content/checkpoints\",\n",
        "                                      save_last=True,\n",
        "                                      save_top_k=1,\n",
        "                                      monitor=\"val_loss\",\n",
        "                                      mode=\"min\",\n",
        "                                      filename=model_name+\"-{epoch:02d}-{val_loss:.5f}\"\n",
        "                                      )\n",
        "stagnate_callback = pl.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                               mode=\"min\",\n",
        "                                               check_finite=True,\n",
        "                                               patience=np.ceil(num_epochs/val_check_interval)\n",
        "                                               )\n",
        "swa_callback = pl.callbacks.StochasticWeightAveraging(swa_lrs=swa_lrs)\n",
        "\n",
        "# Defining trainer\n",
        "trainer = pl.Trainer(accelerator=\"gpu\",             # CPU, GPU or TPU\n",
        "                     val_check_interval=val_check_interval,\n",
        "                     auto_lr_find=False,            # True or False   \n",
        "                     auto_scale_batch_size=None,    # None or \"binsearch\"\n",
        "                     deterministic=False,           # True or False\n",
        "                     fast_dev_run=False,            # True or False or Epoch count\n",
        "                     logger=logger,                 # logger or False\n",
        "                     max_epochs=1000,               # -1 for Infinite\n",
        "                     precision=32,                  # Default 32\n",
        "                     profiler=None,                 # None, \"simple\" or \"advanced\"\n",
        "                     enable_checkpointing=True,     # True or False\n",
        "                     callbacks=[checkpoint_callback, stagnate_callback, swa_callback],\n",
        "                     gradient_clip_val=0.25,\n",
        "                     log_every_n_steps=1\n",
        "                     #, detect_anomaly=True #, overfit_batches=1\n",
        "                     )                  \n",
        "\n",
        "# Autotune hyperparameters\n",
        "trainer.tune(model=model)\n",
        "\n",
        "# Training\n",
        "trainer.fit(model=model)"
      ],
      "metadata": {
        "id": "a2eqAEcNKafw",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluate test set\n",
        "# Garbage collection\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Load best checkpoint and get testing loss\n",
        "#t_loss = trainer.test()[0].get('test_loss')\n",
        "\n",
        "# Get test labels\n",
        "y_test = testset.getData()\n",
        "\n",
        "# Load best checkpoint and get test predictions\n",
        "y_pred = torch.vstack(trainer.predict())"
      ],
      "metadata": {
        "id": "fGt8UkbtXCI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Response distribution\n",
        "# Garbage collection\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Print mean relative errors\n",
        "#print(torch.abs(torch.div(y_pred-y_test,y_test)).mean().item())\n",
        "##print(torch.abs(torch.div(y_pred-y_test,y_pred)).mean().item())\n",
        "\n",
        "# Define RMS histogram binning\n",
        "nb = 20\n",
        "bins1 = np.arange(0, max(torch.max(y_test[:,0]), torch.max(y_pred[:,0])), 1/(2*nb))\n",
        "bins2 = np.arange(0, max(torch.max(y_test[:,1]), torch.max(y_pred[:,1])), 1/(nb/2))\n",
        "bins3 = np.arange(0, max(torch.max(y_test[:,2]), torch.max(y_pred[:,2])), 1/(2*nb))\n",
        "\n",
        "# Plot RMS distributions\n",
        "plt.subplot(1,3,1)\n",
        "plt.hist(y_test[:,0], bins=bins1, color='white', edgecolor='black')\n",
        "plt.hist(y_pred[:,0], bins=bins1, color='blue', alpha=0.6)\n",
        "plt.title(\"Dist RMS MHF\")\n",
        "plt.subplot(1,3,2)\n",
        "plt.hist(y_test[:,1], bins=bins2, color='white', edgecolor='black')\n",
        "plt.hist(y_pred[:,1], bins=bins2, color='blue', alpha=0.6)\n",
        "plt.title(\"Dist RMS BdR\")\n",
        "plt.subplot(1,3,3)\n",
        "plt.hist(y_test[:,2], bins=bins3, color='white', edgecolor='black')\n",
        "plt.hist(y_pred[:,2], bins=bins3, color='blue', alpha=0.6)\n",
        "plt.title(\"Dist RMS MFcirc\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Define error distributions\n",
        "D_mhf = y_pred[:,0]-y_test[:,0] #torch.div(y_pred[:,0]-y_test[:,0],y_pred[:,0])\n",
        "D_bdr = y_pred[:,1]-y_test[:,1] #torch.div(y_pred[:,1]-y_test[:,1],y_pred[:,1])\n",
        "D_mfcirc = y_pred[:,2]-y_test[:,2] #torch.div(y_pred[:,2]-y_test[:,2],y_pred[:,2])\n",
        "\n",
        "# Define error histogram binning\n",
        "nb = 20\n",
        "bins4 = np.arange(-1, 1, 1/nb)\n",
        "bins5 = np.arange(-1, 1, 1/nb)\n",
        "bins6 = np.arange(-1, 1, 1/nb)\n",
        "\n",
        "# Plot error distributions\n",
        "plt.subplot(1,3,1)\n",
        "plt.hist(D_mhf, bins=bins4, color='blue', edgecolor='black')\n",
        "plt.title(\"Erreurs relatives MHF\")\n",
        "plt.subplot(1,3,2)\n",
        "plt.hist(D_bdr, bins=bins5, color='blue', edgecolor='black')\n",
        "plt.title(\"Erreurs relatives BdR\")\n",
        "plt.subplot(1,3,3)\n",
        "plt.hist(D_mfcirc, bins=bins6, color='blue', edgecolor='black')\n",
        "plt.title(\"Erreurs relatives MFcirc\")\n",
        "plt.show()\n",
        "\n",
        "# Print error std\n",
        "std_mhf = torch.std(D_mhf).item()\n",
        "std_bdr = torch.std(D_bdr).item()\n",
        "std_mfcirc = torch.std(D_mfcirc).item()\n",
        "print(f'Standard deviation of MHF errors :    {std_mhf:.3f}')\n",
        "print(f'Standard deviation of BdR errors :    {std_bdr:.3f}')\n",
        "print(f'Standard deviation of MFcirc errors : {std_mfcirc:.3f}')"
      ],
      "metadata": {
        "id": "pFQ-UsQF80CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Single example\n",
        "# Load image and define parameters\n",
        "test_img = cv2.imread(\"/content/map_fit/8bit/-137.319_158.003_37.932_20_30.jpg\",-1)\n",
        "Z = [-137.319,158.003]\n",
        "RMS = 37.932\n",
        "E_RMS = [10,20,30]\n",
        "\n",
        "# Show test image\n",
        "cv2_imshow(test_img)\n",
        "\n",
        "# Normalize image\n",
        "X = np.array(test_img).astype(np.float32)\n",
        "X = (X/255)*(Z[1]-Z[0])+Z[0]\n",
        "X = X/RMS\n",
        "\n",
        "# Define target distribution\n",
        "mean = torch.tensor([0.5, 0.5, 0.5]).view(3,1,1)\n",
        "std = torch.tensor([0.5, 0.5, 0.5]).view(3,1,1)\n",
        "\n",
        "# Define corrected distribution for masked image\n",
        "fact = torch.div(SIZE**2,idx_t.sum(dim=[1,2])).view(3,1,1)\n",
        "corr_mean = torch.mul(fact,mean)\n",
        "corr_std = torch.sqrt(torch.mul(fact,torch.pow(std,2)) - torch.mul(torch.mul(fact,fact-1),torch.pow(mean,2)))\n",
        "\n",
        "# Define transformations\n",
        "lambdaMask = lambda T: torch.mul(T,idx_t)\n",
        "lambdaNorm = lambda T: torch.div(T-T.mean(dim=[1,2]).view(3,1,1),T.std(dim=[1,2]).view(3,1,1))\n",
        "lambdaScale = lambda T: torch.mul(T,corr_std) + corr_mean\n",
        "\n",
        "# Compute transformed image\n",
        "X = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            #transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambdaMask),\n",
        "            transforms.Lambda(lambdaNorm),\n",
        "            transforms.Lambda(lambdaScale),\n",
        "            transforms.Lambda(lambdaMask),\n",
        "            ])(X)\n",
        "\n",
        "# Add dummy batch dimension\n",
        "X = X[None,:,:,:]\n",
        "\n",
        "# Compute prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y = model(X)\n",
        "\n",
        "# Print results\n",
        "print(f' Expected RMS:    {E_RMS[0]:.3f},    {E_RMS[1]:.3f},    {E_RMS[2]:.3f}')\n",
        "print(f'Predicted RMS:    {RMS*y[0,0].item():.3f},    {RMS*y[0,1].item():.3f},    {RMS*y[0,2].item():.3f}')\n",
        "print(f' Error at std:  +-{std_mhf*RMS:.3f},  +-{std_bdr*RMS:.3f},  +-{std_mfcirc*RMS:.3f}')"
      ],
      "metadata": {
        "id": "aBbcZe7prq22",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Export to .onnx\n",
        "#@markdown Errors on this block are mostly fixed by restarting it\n",
        "# Target filename\n",
        "filename = f'C:/Users/maele/Desktop/map_fit-main/Export/{model_name}_{std_mhf:.3f}_{std_bdr:.3f}_{std_mfcirc:.3f}.onnx' # drive/MyDrive/ \n",
        "\n",
        "# Example input\n",
        "x = torch.randn(1, 3, SIZE, SIZE, requires_grad=True)\n",
        "\n",
        "# Export the model to Open Neural Network eXchange (ONNX)\n",
        "model.to_onnx(filename, x, export_params=True)"
      ],
      "metadata": {
        "id": "v45W0Eh1nvRp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}