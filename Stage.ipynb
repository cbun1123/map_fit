{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Z4cFHcvo_An1N5BwmUgAvlcDgpPVOuce",
      "authorship_tag": "ABX9TyPojFy6bywgzt2UW+f05az8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbun1123/map_fit/blob/main/Stage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/cbun1123/map_fit\n",
        "\n",
        "!unzip /content/map_fit/X_test_1.zip -d /content/map_fit/X_test\n",
        "!unzip /content/map_fit/X_test_2.zip -d /content/map_fit/X_test\n",
        "!unzip /content/map_fit/X_test_3.zip -d /content/map_fit/X_test\n",
        "\n",
        "!rm /content/map_fit/*.zip\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "%env PYTHONPATH=.:$PYTHONPATH"
      ],
      "metadata": {
        "id": "0mtWihytt32k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ow-HQUN3k9SA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d612f4a-8e9c-4773-949c-a2e4e1ddeed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 19 09:21:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import math\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "Gj6-PAXEtQDT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = glob.glob(\"/content/map_fit/X_test/*.jpg\")\n",
        "filenames.sort()\n",
        "images = [cv2.imread(img) for img in filenames]\n",
        "\n",
        "X_train = np.stack(images, axis=0)\n",
        "y_train = np.load('/content/map_fit/Y_test.npy')\n",
        "print(np.shape(X_train))\n",
        "\n",
        "SIZE = 224\n",
        "\n",
        "lin = np.linspace(-1,1,SIZE)\n",
        "[Xm,Ym] = np.meshgrid(lin,lin)\n",
        "idx = ((Xm**2+Ym**2)<1)\n",
        "idx = np.stack([idx,idx,idx], axis=0)\n",
        "idx_t = torch.from_numpy(idx)\n",
        "\n",
        "class imageDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        'Initialization'\n",
        "        self.X = X_train\n",
        "        self.y = y_train\n",
        "        self.n_samples = self.X.shape[0]\n",
        "        self.transform = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        X = self.X[index]\n",
        "        X = self.transform(X)\n",
        "        X = torch.mul(X,idx_t)\n",
        "\n",
        "        y = self.y[index]\n",
        "        y = torch.from_numpy(y)\n",
        "        sample = X,y\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "\n",
        "dataset = imageDataset()\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "0CMP3MWXNj-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee04eda0-2799-479d-a7d3-e3631a070966"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# model init\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "\n",
        "# hyperparameters\n",
        "num_epochs = 2\n",
        "learning_rate = 0.01\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(, lr=learning_rate)\n",
        "\n",
        "\n",
        "total_samples = len(dataset)\n",
        "n_iteration = math.ceil(total_samples/32)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i,(inputs,labels) in enumerate(dataloader):\n",
        "        # Trucs\n",
        "        if (i % 10) == 0:\n",
        "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iteration}, inputs {inputs.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl6yhwY9coxM",
        "outputId": "82a4ae06-1904-4e2e-f946-4b0c978f540f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/2, step 1/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 1/2, step 11/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 1/2, step 21/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 1/2, step 31/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 1/2, step 41/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 1/2, step 51/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 1/2, step 61/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 1/2, step 71/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 1/2, step 81/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 1/2, step 91/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 2/2, step 1/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 2/2, step 11/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 2/2, step 21/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 2/2, step 31/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 2/2, step 41/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 2/2, step 51/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 2/2, step 61/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 2/2, step 71/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 2/2, step 81/94, inputs torch.Size([32, 3, 224, 224])\n",
            "epoch 2/2, step 91/94, inputs torch.Size([32, 3, 224, 224])\n"
          ]
        }
      ]
    }
  ]
}